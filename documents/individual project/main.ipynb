{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Subject: Machine Learning, 19th Feb 2024 - 29th March 2024.](#toc0_)\n",
    "### <a id='toc1_1_1_'></a>[Topic: Individual Assessment.](#toc0_)\n",
    "- Research Purpose: Using real-world data sets on supervised learning models to evaluate the classification of two algorithms.\n",
    "\n",
    "### <a id='toc1_1_2_'></a>[Learning Outcomes:](#toc0_)\n",
    "- MO1: Compare and contrast the basic principles and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Subject: Machine Learning, 19th Feb 2024 - 29th March 2024.](#toc1_)    \n",
    "    - [Topic: Individual Assessment.](#toc1_1_1_)    \n",
    "    - [Learning Outcomes:](#toc1_1_2_)    \n",
    "- [Import libraries to run the project:](#toc2_)    \n",
    "- [Analysis and treatment of dataset (10%):](#toc3_)    \n",
    "  - [Model Analysis](#toc3_1_)    \n",
    "    - [SVM](#toc3_1_1_)    \n",
    "    - [Ensemble](#toc3_1_2_)    \n",
    "    - [Links:](#toc3_1_3_)    \n",
    "  - [Data collection and preprocessing:](#toc3_2_)    \n",
    "    - [Find missing feature values using missingno](#toc3_2_1_)    \n",
    "    - [Fill missing values with imputing](#toc3_2_2_)    \n",
    "    - [Feature Scaling (Standardisation):](#toc3_2_3_)    \n",
    "    - [Transform data](#toc3_2_4_)    \n",
    "    - [Show the usefulness of the data via a correlation matrix](#toc3_2_5_)    \n",
    "    - [Create the test and training sets](#toc3_2_6_)    \n",
    "- [Model and Training (40%):](#toc4_)    \n",
    "  - [Creating our SVM Model](#toc4_1_)    \n",
    "    - [Using SVC with Grid Search CV](#toc4_1_1_)    \n",
    "    - [Get the best parameters from Grid Search CV](#toc4_1_2_)    \n",
    "  - [Creating our ensemble Model](#toc4_2_)    \n",
    "      - [Using Random Forest ensemble with AdaBoost](#toc4_2_1_1_)    \n",
    "- [Prediction and Evaluation (30%):](#toc5_)    \n",
    "  - [Evaluating the model with Confusion Matrices](#toc5_1_)    \n",
    "  - [Predicting and evaluating the SVM](#toc5_2_)    \n",
    "    - [Understanding SVM data with Confusion Matrices](#toc5_2_1_)    \n",
    "      - [Evaluate SVM performance metrics using Confusion Matrix](#toc5_2_1_1_)    \n",
    "  - [Predicting and evaluating the Ensemble](#toc5_3_)    \n",
    "      - [Evaluate Ensemble performance metrics using Confusion Matrix](#toc5_3_1_1_)    \n",
    "- [Comparing our SVM and Ensemble](#toc6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Import libraries to run the project:](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook is part of the Individual Assessment.\n",
    "It contains two supervised learning models with information\n",
    "about their use-case and the understanding of different\n",
    "classifications of real-world datasets comparatively.\n",
    "\n",
    "Originally made by Reece Turner, 22036698.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import big_o\n",
    "import logging # Disable lag from output\n",
    "\n",
    "logger = logging.getLogger('requests_throttler')\n",
    "logger.addHandler(logging.NullHandler())\n",
    "logger.propagate = False\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier\n",
    ")\n",
    "\n",
    "# Active root directory of the project folder\n",
    "current_directory = os.path.dirname(os.path.abspath(__name__))\n",
    "current_directory += \"\\\\\"\n",
    "\n",
    "# Constant Variables\n",
    "C_HYPERPARAMETER = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Analysis and treatment of dataset (10%):](#toc0_)\n",
    "**Explain and justify the selection of models. Describe the training and the appropriateness of hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to treat the dataset before the SVM and ensemble can use it. Analysing and treating a dataset involves a range of subtasks that are commonly found in Machine Learning. At the most basic form these involve:\n",
    "\n",
    "- Domain Knowledge - Understanding the data.\n",
    "- Data Cleaning - Handle missing values.\n",
    "- Feature Engineering - Creating or transforming features to improve model performance.\n",
    "- Normalisation/Standardisation - Places all the data into similar ranges to improve the dataset.\n",
    "\n",
    "**_NOTE:_** Some of the bullet points have subtasks and are dependent on the type of models. An SVM may require Feature Scaling (Standardisation) whereas an ensemble does not due its decision tree based design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Model Analysis](#toc0_)\n",
    "**Describe, explain and justify your approach to building each of the two models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[SVM](#toc0_)\n",
    "An SVM is a Machine Learning Algorithm built on supervision with the use of classification or regression to group inputs, also known as features, into their classified support vectors. An extension of this implements kernels to the machine to classify features. These kernels are Radial Basis Function (RBF), non-linear and linear. While Non-linear and RBF can address higher dimensional space, linear can create decision boundaries, called hyperplanes, for input features, $x$, where $H: {x|w^Tx+b = 0}$. The justification of potentially using linear means we can challenge its application and whether or not its comparatively any better than our ensemble and if the potential use is worth the complexity. \n",
    "\n",
    "In regards to our selected linear model, the design and implementation will consist of a pre-defined hyperparamter, C, thats trained on with the use of Grid Search Cross validation. This exhaustive search algorithm will fit the SVM to it and should return an optimal hyperparameter solution so help us determine the accuracy and precision scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Ensemble](#toc0_)\n",
    "Ensembles on the other hand are also a supervised learning model with the exception they are built on multiple base learning using decision trees such as Random Forest. Additionally, they can incoroporate ML boosting techniques like XGBoost and AdaBoost (Adapative Boosting) to improve their time complexity and accuracy. Comparatively to SVM, performance is calculated differently on ensembles because feature scaling (normalisation) is not used as algorithm examples like Random Forest or gradient boosting split criteriion into 'stumps' for each input based on their comparative feature values.\n",
    "\n",
    "Regarding ensembles, we are going to implement Random Forest classification as our base classifier with the settings of a 'weak learner' then train it on an Adaptive Boosting model as the 'strong learner' so that we can achieve high predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[Links:](#toc0_)\n",
    "\"Does Random Forest Need Feature Scaling or Normalization?\" (2023) forecastegy.com. [online] Available from: https://forecastegy.com/posts/does-random-forest-need-feature-scaling-or-normalization/#:~:text=Random%20Forest%20is%20a%20tree,can%20be%20skipped%20during%20preprocessing [Accessed 27th March 2024]\n",
    "\n",
    "\"Lecture 3: The Perceptron\" (2024) cornell.edu. [online] Available from: https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote03.html [Accessed 20th March 2024]\n",
    "\n",
    "\"Lecture 9: SVM\" (2024) cornell.edu. [online] Available from: https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote09.html [Accessed 20th March 2024]\n",
    "\n",
    "https://www.youtube.com/watch?v=NnmKeYUYMPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Data collection and preprocessing:](#toc0_)\n",
    "In this section we want to identify which columns in our dataframe are features and which one is the target variable. The reason for providing an $X$ and $y$ is so the model can be trained on only the features while not providing it the actual outcome which would inherently defeat the purpose of supervision.\n",
    "\n",
    "Comparatively, the models performance is effected by this step in respects to their design. SVMs will have a higher performance if there is no redundent data therefore the use of imputing is required to make sure the data has completeness whereas ensembles are not as sensitive to this occurance so they can make decisions without. This addressed, its not harmful to both algorithms when using imputing therefore we will handle all missing values and follow their respective model processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(current_directory + \"dataset\\diabetes.csv\")\n",
    "X_features = data.drop(columns=[\"Outcome\"])\n",
    "y_target = data[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Find missing feature values using missingno](#toc0_)\n",
    "The most probable missing features are:\n",
    "- BMI\n",
    "- Glucose\n",
    "- Blood Pressure\n",
    "\n",
    "We only check these in our data set for '0' because they are likely to be dead or non existent. Moreover, The unspecified features do not directly correlate to diabeties and act more as an evaluation of if someone is. An instance where this occurs is the Diabetes Pedigree Function where 0 is considered healthy and 1 is considered diabetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature columns\n",
    "cols = [\"Glucose\", \"BloodPressure\", \"BMI\", \"Age\"]\n",
    "missing_values = X_features\n",
    "missing_values[cols] = missing_values[cols].replace(0, np.nan)\n",
    "\n",
    "# Create a matrix of features to observe missing ones.\n",
    "# Any horizontal lines within the columns indicate a missing value.\n",
    "# The vertical line on the right of the plot indicates data completeness.\n",
    "msno.matrix(missing_values, figsize=(6, 4))\n",
    "plt.title(\"Matrix of missing cell values\")\n",
    "plt.ylabel(\"Index\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Fill missing values with imputing](#toc0_)\n",
    "Now that we identified which columns produce values with invalid data, we can now address this using a technique called imputing. To perform imputing we need to build an Imputer object that can fit and transform our missing values then from there this will become our X_features dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iterative imputing and refine results over x times.\n",
    "# This can be beneficial for data accuracy but iterations\n",
    "# on large datasets can increase time complexity.\n",
    "imputer = IterativeImputer(\n",
    "    max_iter=10,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Fit and transform the dataset\n",
    "# This now becomes our features for train test split.\n",
    "X_features = imputer.fit_transform(missing_values)\n",
    "X_features = pd.DataFrame(X_features, columns=missing_values.columns)\n",
    "# X_features.to_csv(\"imputed_diabetes.csv\", index=False)\n",
    "\n",
    "# Graph the updated features\n",
    "msno.matrix(X_features, figsize=(6, 4))\n",
    "plt.title(\"Matrix of missing cell values\")\n",
    "plt.ylabel(\"Index\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[Feature Scaling (Standardisation):](#toc0_)\n",
    "Now that we have data completeness, we can address the SVM data handling via standarisation using sklearns Standard Scaler.\n",
    "\n",
    "**_Note_**: It's worth mentioning that a pipeline is normally used for this process as it lets you implement a range of scaling techniques like chaining preprocesses such as classifiers and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standard scaler for our features.\n",
    "# For our configuration we have no additional\n",
    "# pipeline steps therefore a simple object should suffice.\n",
    "svm_scaler = StandardScaler()\n",
    "svm_scaler.fit(X_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_4_'></a>[Transform data](#toc0_)\n",
    "After creation, we need to transform the features to prevent overhead. This is performed by the transform function. The function uses ML preprocessing techniques called centering and scaling for this particular standardisation. From there this will become our SVM standardised data.\n",
    "\n",
    "Once data completeness has been obtained we need to transform our features into appropriate ranges our models using centering and scaling. \n",
    "\n",
    "$transform(X) : y$ s.t. $x$ is the centered and scaled features and $y$ is the returned transformed features.\n",
    "- Centering substracts the mean of each feature from all values in the column.\n",
    "- Scaling adjusts the range of values of each feature. This can either involve z-score scaling or min-max scaling.\n",
    "\n",
    "**_NOTE_**: sklearn will handle this part of scaling through the use of their scaler objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform our scaled data\n",
    "standardised_data = svm_scaler.transform(X_features)\n",
    "\n",
    "# Our previous features are transformed and now became svm features.\n",
    "svm_features = standardised_data\n",
    "\n",
    "# Output the transformed data\n",
    "print(svm_features, \"\\n\", np.array(y_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_5_'></a>[Show the usefulness of the data via a correlation matrix](#toc0_)\n",
    "We are going to use seaborn to display this data. This will help us identify the relationships between the variables to see how the relate to each other. A correlation matrix is an alternative to a hyperplane classification for when there is more than 2 features in the dataset. Visualising this way lets us classifiy relationships on a 2-axis matrix where we can view the importance between features, $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix and display it\n",
    "correlation_matrix = X_features.corr()\n",
    "plt.figure(figsize=(16, 9))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    fmt=\".2f\" # Floating points\n",
    ")\n",
    "\n",
    "plt.title(\"Correlation Matrix of relationship appropriateness.\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the cell generates a graph which clearly outlines which features respect or disrespect their pairs. An example of this occuring is Age and Pregnancies having a middle-ground relationship where as the red boxes indicate a strong relationship - probably because they are relating to themselves. This understanding of the features displays insight into the potential performance of the algorithm before train test split is used since the data with the least relationships tend to effect the accuracy later.\n",
    "\n",
    "**_NOTE_**: Strong Positive values indicate a direct correlation of relationship and vise versa for negative correlation. If the value is 0 or close then theres no or little relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_6_'></a>[Create the test and training sets](#toc0_)\n",
    "Since this algorithm is supervised we need to give the testing data a partial amount of the holistic data so we can assess its score. Conversely, the remaining data becomes the training data.\n",
    "\n",
    "![Logo](https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/1_train-test-split_0.jpg)\n",
    "\n",
    "Based on the graphic above we will perform a 70-30 split on the data. Doing this means we have a large enough dataset to train the model on whilst ensuring theres enough unseen data for it to learn in the future. This train test split ratio will be beneficial to both algorithms since the trained data helps the model manage unprecedented feature (test) data whether it's insignificant or extreme.\n",
    "\n",
    "\"Understanding Train Test Split\" (2022) builtin.com. [online] Available from: https://builtin.com/data-science/train-test-split [Accessed 25th March 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data and keep random state to 0 so we can produce replicable results\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    svm_features,  # Use the standardised features rather than the original X_features\n",
    "    y_target,\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "features_shape = svm_features.shape[0]\n",
    "X_train_shape = X_train.shape[0]\n",
    "X_train_perc = (X_train.shape[0]/svm_features.shape[0])*100\n",
    "X_test_shape = X_test.shape[0]\n",
    "X_test_shape = (X_test.shape[0]/svm_features.shape[0])*100\n",
    "print(\n",
    "    f\"Original Features {features_shape} - 100% of the original set.\",\n",
    "    f\"\\nX_train Features {X_train_shape}  - {X_train_perc}% of the original set.\",\n",
    "    f\"\\nX_test Features {X_test_shape}   - {X_test_shape}% of the original set.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Model and Training (40%):](#toc0_)\n",
    "**Explain and justify the selection of models. Describe the training process and the appropriate selection of hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Creating our SVM Model](#toc0_)\n",
    "Here we create a SVM Classifier Model to fit our data. Since this is our first initialisation of the model we are unaware what the new best hyperparameters are so we assume our constant is the current best. From there the model will be extend off the hyperparameter into a larger search space in order to optimise further - we will use Grid Search CV for this. Our expected result of this process should indicate the model has improved in performance.\n",
    "\n",
    "Our training for this model will consist of:\n",
    "- Kernel Functions\n",
    "- Classification Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Support Vector Classifier\n",
    "# Later we can implement some techniques to optimise this in searches.\n",
    "svm_classifier = svm.SVC(\n",
    "    kernel=\"linear\",\n",
    "    random_state=0,\n",
    "    C=C_HYPERPARAMETER\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the SVM model according to the given training data. If we do not implement a grid search then fitting here instead is required, otherwise passed into grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to compare between the intial train instance vs any farther analytics.\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "base_svm_y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Output the accuracy of the model training before grid search.\n",
    "print(\n",
    "    f\"The intial C hyperparameter of {C_HYPERPARAMETER} generates an accuracy of \"\n",
    "    + f\"{accuracy_score(y_test, base_svm_y_pred)*100} %.\"\n",
    "    \"\\nC param cannot be discovered right away due to human intervention therefore \"\n",
    "    + \"we assume there is room for performance improvement.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: Testing the accuracy of the base model is a good way to generalise improvement. If the score doesn't seem particularly high it indicates there is still a large remainder of inaccurate results - this is where grid search comes in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Using SVC with Grid Search CV](#toc0_)\n",
    "\n",
    "The Grid Search CV is a cross-validation exhaustive search algorithm. It goes through a range of parameters and checks for the best hyperparameter configuration within the intial help of human intervention. Typically, a Grid Search CV should include a kernel, random state and a hyperparameter to obtain the desired output therefore to make it a fair test we will keep random state and kernel the same but the Classifier variable, $C$, will have a range depending on the size of the dataset. In our case the dataset is $\\lt 1000$ so we can use $[C_1, 0.1, 1, 10, 100, 1000]$ incrementation for our sample but it should be noted that ranges too large will generate noise and this will lead to problems regarding overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid search params. If C constant is not in\n",
    "# in the grid search then add it to the array.\n",
    "c_params = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "if C_HYPERPARAMETER not in c_params:\n",
    "    c_params = [C_HYPERPARAMETER] + c_params\n",
    "\n",
    "# We are not using gamma since our kernel is not RBF.\n",
    "param_grid = {\n",
    "    \"C\": c_params,\n",
    "    \"kernel\": ['linear']\n",
    "}\n",
    "\n",
    "# The number of subset (folds) from the current set.\n",
    "# This is typically a better method for checking accuracy as it\n",
    "# creates a meta-estimator from the single estimator model for our training.\n",
    "# Using less extreme numbers prevent overfitting.\n",
    "cross_validation = 5\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_classifier,\n",
    "    param_grid=param_grid,\n",
    "    cv=cross_validation,\n",
    "    scoring=\"accuracy\"  # Forces classification instead of regression.\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the training data to the grid search model so that we can analyse performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Get the best parameters from Grid Search CV](#toc0_)\n",
    "Running the cell below will return performance metrics about SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_classifier = grid_search.best_estimator_\n",
    "best_svm_params = grid_search.best_params_\n",
    "\n",
    "# This metric is the 'cv' performance of the machine learning model.\n",
    "best_svm_score = grid_search.best_score_\n",
    "\n",
    "print(\n",
    "    \"****** TRAINING DATA ******\",\n",
    "    \"\\nSVM Grid Search best (mean) score: %.6f\" % (best_svm_score*100) + \" %\",\n",
    "    f\"\\nSVM Grid Search best current estimator: {best_svm_classifier}.\"\n",
    "    f\"\\nSVM Grid Search best current parameters: {best_svm_params}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: The best hyperparameters from the solution are potentially not the global optimum of every possible combination. To find this it would dramatically increase the search space and time complexity which is out of range for our use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Creating our ensemble Model](#toc0_)\n",
    "Create a Ensemble Random Forest base classifier then a Adapative Boost classifier to train on.\n",
    "\n",
    "To make our Random Forest a weak learner we have to set its hyperparameters to low depth searching properties, vise versa for strong learners. Restricting the complexity of its learning will reduce the models risk of overfitting which is helpful for our high dimensional dataset.\n",
    "\n",
    "\"Random Forest and Boosting\" (2022) cornell.edu [online] Available from: https://www.cs.cornell.edu/courses/cs4780/2022sp/notes/Notes22.pdf [Accessed 27th March 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base random forest classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    random_state=0,\n",
    "    max_features=\"log2\",\n",
    ")\n",
    "\n",
    "# Create a ada boost classifier and pass in the single\n",
    "# estimator/model.\n",
    "ab_classifier = AdaBoostClassifier(\n",
    "    estimator=rf_classifier,\n",
    "    n_estimators=100,\n",
    "    random_state=0,\n",
    "    algorithm=\"SAMME\"  # Using SAMME since SAMME.R is deprecated.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc4_2_1_1_'></a>[Using Random Forest ensemble with AdaBoost](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to the model\n",
    "ab_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Prediction and Evaluation (30%):](#toc0_)\n",
    "**Generate and provide clear analysis, explaination and justification of evaluating methods and performance metrics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Evaluating the model with Confusion Matrices](#toc0_)\n",
    "Since our dataset target variable is base 2, this means it is Binary classification. Binary classification tells us if the data is either True or False and this technique can be useful for visualisation on a Confusion Matrix. To implement this Matrix we can use a library similar to our preprocessing called missingno with the implementation of a heatmap.\n",
    "\n",
    "![Logo](https://cdn.discordapp.com/attachments/1165954528269574194/1223136626738200576/accuracy_calc.PNG?ex=6618c1c0&is=66064cc0&hm=c6e7a4e6925e30822caacea07970006825570671e21ea4ca30f3dc853f9bccf8&)\n",
    "\n",
    "*Note: in binary classification, the count of true negatives is\n",
    "C_{0,0}, false negatives is C_{1,0}, true positives is\n",
    "C_{1,1} and false positives is C_{0,1} - scikit-learn*\n",
    "\n",
    "This matrix is designed to show prediction against the actual values of the data in terms of True/False Negatives or positives. This type of matrix will visualise areas where the model lacked accuracy. \n",
    "\n",
    "- Accuracy being the percentage of correct predictions on a train or test dataset. <br>\n",
    "- Precision is the ratio between True Positives and all the Positives. <br>\n",
    "- Recall measures our model to determine correctly identified True Positives. <br>\n",
    "- Specificity measures our model to determine correctly identified True Negatives.\n",
    "\n",
    "$Accuracy = {\\frac{\\sum TP + TN}{\\sum TP + FP + FN + TN}}$ <br>\n",
    "\n",
    "$Precision$ = ${\\frac{\\sum TP}{\\sum TP + FP}}$ <br>\n",
    "\n",
    "$Recall$ = $\\frac{\\sum TP}{\\sum TP + FN}$\n",
    "\n",
    "$F$(1) $Score$ = $2 \\times \\frac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "$Specificity$ = $\\frac{TN}{TN + TP}$\n",
    "\n",
    "**_NOTE_**: The type of interpreted data is important for deciding if the f1 score is leveraged correctly. In our case, we should prioritise recall more since we are dealing with medical records and we want more True Positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_2_'></a>[Predicting and evaluating the SVM](#toc0_)\n",
    "Here we can produce the results of the test data. Using sklearn we want to obtain the classification report of $y$, s.t. $predict(x): y$ where $x$ is the input data (X_test). A classification report tells us our accuracy, precision and recall scores which in affect helps optimise the model further.\n",
    "\n",
    "If the report tells us that the stats are 100% then its likely to have identical data as to what it was trained with which would *normally* indicate an issue. It is more plausible that a trained data fit score will likely fail as it's not seen the data before. As a result of this judgement its imperative that we train the model first then accuracy score the model on the test data afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model Test data accuracy\n",
    "# y_test_pred2 = svm_classifier.predict(X_test)\n",
    "\n",
    "# Use the SVM Grid search best hyperparameter as our fitting\n",
    "# print(features)\n",
    "svm_y_test_pred = best_svm_classifier.predict(X_test)\n",
    "\n",
    "svm_test_accuracy = accuracy_score(\n",
    "    y_test,\n",
    "    svm_y_test_pred\n",
    ")\n",
    "\n",
    "# SVM Model Test data precision score\n",
    "svm_test_precision = precision_score(\n",
    "    y_test,\n",
    "    svm_y_test_pred,\n",
    "    average=\"weighted\"\n",
    ")\n",
    "\n",
    "# This is the result of the models accuracy from training data\n",
    "# then testing new data on the model after to see how it responds.\n",
    "# That explains why doing it without grid search displays the same result.\n",
    "print(\n",
    "    \"****** SVM TEST DATA ******\",\n",
    "    f\"\\n{best_svm_classifier}\",\n",
    "    \"\\nAccuracy score: \" + \"%.6f\" % (svm_test_accuracy*100) + \" %\",\n",
    "    \"\\nPrecision score: \" + \"%.6f\" % (svm_test_precision*100) + \" %\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual values vs Predicted values\n",
    "actual_values = np.array(y_test)\n",
    "pred_values = np.array(svm_y_test_pred)\n",
    "print(actual_values)\n",
    "print(pred_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: The actual values are the original outcome column and the predicted values are what the model predicted using the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_2_1_'></a>[Understanding SVM data with Confusion Matrices](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix using the prediction and test values\n",
    "def get_confusion_matrix(actual_values, predicted_values):\n",
    "    \"\"\"\n",
    "    Create a sklearn confusion matrix.\n",
    "\n",
    "    In binary classification, the count of true negatives is\n",
    "    C_{0,0}, false negatives is C_{1,0}, true positives is\n",
    "    C_{1,1} and false positives is C_{0,1}.\n",
    "    \"\"\"\n",
    "\n",
    "    return confusion_matrix(\n",
    "    y_true=actual_values,\n",
    "    y_pred=predicted_values\n",
    ")\n",
    "\n",
    "con_matrix = get_confusion_matrix(actual_values, pred_values)\n",
    "\n",
    "# Find the values where its incorrect\n",
    "false_positives = np.where((actual_values == 0) & (pred_values == 1))\n",
    "false_negatives = np.where((actual_values == 1) & (pred_values == 0))\n",
    "true_positives = np.where((actual_values == 1) & (pred_values == 1))\n",
    "true_negatives = np.where((actual_values == 0) & (pred_values == 0))\n",
    "\n",
    "# Obtain the respective number of tn, fp, fn, tp.\n",
    "tn, fp, fn, tp = con_matrix.ravel()\n",
    "print(\n",
    "    f\"\\nTN: {tn}\",\n",
    "    f\"\\nFN: {fn}\",\n",
    "    f\"\\nTP: {tp}\",\n",
    "    f\"\\nFP: {fp}\",\n",
    ")\n",
    "\n",
    "# print(\"Indexes of False Positives:\", false_positives[0])\n",
    "# print(\"Indexes of False Negatives:\", false_negatives[0])\n",
    "\n",
    "sns.heatmap(\n",
    "    con_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    xticklabels=[\"Negative\", \"Positive\"],\n",
    "    yticklabels=[\"Negative\", \"Positive\"],\n",
    ")\n",
    "\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: Our diabetes-description.txt file suggests: \"Outcome (1 is interpreted as \"tested positive for diabetes\" and 0 as \"negative\")\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_instances = tn + fp + fn + tp\n",
    "percentage_TP = (tp / total_instances) * 100\n",
    "percentage_FP = (fp / total_instances) * 100\n",
    "percentage_TN = (tn / total_instances) * 100\n",
    "percentage_FN = (fn / total_instances) * 100\n",
    "\n",
    "print(f\"True Positive (TP): {percentage_TP} %\")\n",
    "print(f\"False Positive (FP): {percentage_FP} %\")\n",
    "print(f\"True Negative (TN): {percentage_TN} %\")\n",
    "print(f\"False Negative (FN): {percentage_FN} %\")\n",
    "\n",
    "correct_values = len(y_test) - fp - fn\n",
    "incorrect_values = len(y_test) - tp - tn\n",
    "print(\n",
    "    \"\\nBased on the findings, the SVM model is able to \"\n",
    "    + f\"predict {correct_values} correct values.\",\n",
    "    f\"\\nOn the other hand, the SVM is incorrectly \"\n",
    "    + f\"guessing {incorrect_values} values.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data suggests that the model is typically able to find most of the target variable values through prediction but lacks understanding of some guessed values. This potentially means that the data for these $y$ rows may be incorrect feature values. Alternatively the configuration of the SVM can also impact its performance on the dataset because of the kernel type and while linear kernel is good for high dimensional spaces, the implementation of i.e. Radial Basis Function may produce a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_2_1_1_'></a>[Evaluate SVM performance metrics using Confusion Matrix](#toc0_)\n",
    "\n",
    "Using the recall function, we can accurately show which values are genuine regarding True Positives (TP) and True Negatives (TP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage how many positives are factual\n",
    "recall: float = recall_score(actual_values, pred_values)\n",
    "\n",
    "# Get the percentage how many negatives are factual\n",
    "specificity: float = con_matrix[0, 0] / (con_matrix[0, 0] + con_matrix[0, 1])\n",
    "\n",
    "print(\"Model recall score: %.6f\" % (recall*100) + \" %\")\n",
    "print(\"Model specificity score (opposite of recall): %.6f\" % (specificity*100) + \" %\")\n",
    "\n",
    "print(\"Note: A 50/50 split would be considered desired in this scenario.\")\n",
    "difference = specificity - recall\n",
    "percentage_difference = (difference / recall) * 100\n",
    "\n",
    "# Provide a model_accuracy_allowance as the result is unlikely to be exact precision\n",
    "model_accuracy_allowance = 5\n",
    "\n",
    "if percentage_difference > model_accuracy_allowance:\n",
    "    print(\n",
    "        \"\\nIn our evaluation it seems that the difference \"\n",
    "        + \"between true negatives and true positives is \"\n",
    "        + \"%.6f\" % (percentage_difference) + \" %.\"\n",
    "        + \"\\nThis would indicate its likely the prediction \"\n",
    "        + \"may need optimisation and is overfitting.\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        f\"\\nIt seem that the the difference of {difference} \"\n",
    "        + \"is within the {percentage_difference} percentage difference.\"\n",
    "        + \"\\nTherefore the model is predicting accurately.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_3_'></a>[Predicting and evaluating the Ensemble](#toc0_)\n",
    "To predict and evaluate the esemble we can use classification techniques simialar to our Support Vector Classifier. The criterion we focus on the most will be f1-score and accuracy since our data based on the previous model confusion matrix illustrates some incorrect values. We want to optimise our performance further and demonstrate which model design is most appropriate for our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_y_pred = ab_classifier.predict(X_test)\n",
    "\n",
    "ab_test_accuracy = accuracy_score(\n",
    "    y_test,\n",
    "    ab_y_pred\n",
    ")\n",
    "\n",
    "ab_test_precision = precision_score(\n",
    "    y_test,\n",
    "    ab_y_pred,\n",
    "    average=\"weighted\"\n",
    ")\n",
    "\n",
    "f1 = f1_score(actual_values, ab_y_pred)\n",
    "\n",
    "print(\n",
    "    \"****** SVM TEST DATA ******\",\n",
    "    f\"\\n{ab_classifier}\",\n",
    "    \"\\nAccuracy score: \" + \"%.6f\" % (ab_test_accuracy*100) + \" %\",\n",
    "    \"\\nPrecision score: \" + \"%.6f\" % (ab_test_precision*100) + \" %\",\n",
    "    \"\\nF-1 score: \" + \"%.6f\" % (f1*100) + \" %\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_perc_diff = abs((svm_test_accuracy - ab_test_accuracy) / ab_test_accuracy) * 100\n",
    "\n",
    "if ab_test_accuracy > svm_test_accuracy:\n",
    "    print(\n",
    "        \"Comparing the Ensemble model accuracy against SVM shows:\",\n",
    "        \"\\nAccuracy increase of \", accuracy_perc_diff, \" %\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: Accuracy and precision alone does not indicate true performance; because of this we can check how the Ensemble performes using a f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc5_3_1_1_'></a>[Evaluate Ensemble performance metrics using Confusion Matrix](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a confusion matrix to relate our f1 score\n",
    "con_matrix = get_confusion_matrix(actual_values, ab_y_pred)\n",
    "categories = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "\n",
    "# Obtain the respective number of tn, fp, fn, tp.\n",
    "tn, fp, fn, tp = con_matrix.ravel()\n",
    "print(\n",
    "    f\"\\nTN: {tn}\",\n",
    "    f\"\\nFN: {fn}\",\n",
    "    f\"\\nTP: {tp}\",\n",
    "    f\"\\nFP: {fp}\",\n",
    ")\n",
    "\n",
    "sns.heatmap(\n",
    "    con_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    xticklabels=[\"Negative\", \"Positive\"],\n",
    "    yticklabels=[\"Negative\", \"Positive\"],\n",
    ")\n",
    "\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_NOTE_**: Our diabetes-description.txt file suggests: \"Outcome (1 is interpreted as \"tested positive for diabetes\" and 0 as \"negative\")\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_instances = tn + fp + fn + tp\n",
    "percentage_TP = (tp / total_instances) * 100\n",
    "percentage_FP = (fp / total_instances) * 100\n",
    "percentage_TN = (tn / total_instances) * 100\n",
    "percentage_FN = (fn / total_instances) * 100\n",
    "\n",
    "print(f\"Percentage of True Positive (TP): {percentage_TP} %\")\n",
    "print(f\"Percentage of False Positive (FP): {percentage_FP} %\")\n",
    "print(f\"Percentage of True Negative (TN): {percentage_TN} %\")\n",
    "print(f\"Percentage of False Negative (FN): {percentage_FN} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of our confusion matrix shows how our Ensemble is performing regarding predicting the correct target variable. In our train test split 30 percent of the data is being used in our confusion matrix and out of this data $TP + TN$ is factually correct. Calculating the percentage of all the values gives us an insight into the accuracy of the model and this shows us that $FP + FN$ are incorrectly predicted demonstrating a room for improvement. Such model adjustments could involve using kernel tricks for high dimensional classes or changing the max_features parameter from $log2$ to $sqrt$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Comparing our SVM and Ensemble](#toc0_)\n",
    "In regards to both models and their performance metrics its noticable their different applications can induce anomalies respectively. These anomalies are usually produced by how the model interprets the dimensional factors for classification which are shown throughout this document. Our SVM was linear which can encompass high dimensional spaces but at the cost of lower training time complexity, $O(nSamples \\times nFeatures)$ compared to our Ensemble Random Forest configuration $O(nSamples \\times nFeatures \\times nEstimators \\times log(nSamples))$ plus Adaptive Boost $O(nEstimators \\times weakLearnerComplexity)$. However, we configured Random Forest via AdaBoost to add extra layers on our model to enable weak and strong learning whereas the SVM is a strong learner by default. This is advantageous because we can reduce the maximum depth of learning for Random Forest preventing overfitting while implementing Adaptive boosting to reduce bias by giving weight, $w$, to missclassified data points but as a result we increase time complexity with the trade off for higher accuracy.\n",
    "\n",
    "SVM & Random Forest variables:\n",
    "- n_samples is the count of samples in the dataset.\n",
    "- n_features is the count of features in the dataset.\n",
    "\n",
    "Random Forest variables:\n",
    "- n_estimators is the number of trees (stumps) in a forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
