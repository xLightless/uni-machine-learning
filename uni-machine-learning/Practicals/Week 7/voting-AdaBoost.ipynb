{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 7\n",
    "\n",
    "## Tutorial 1. Voting\n",
    "\n",
    "The objectives of this practical are to learn:\n",
    "\n",
    "- the core concepts of voting classifiers\n",
    "- the core concepts of adaptive boosting\n",
    "- how to tune the parameters in a AdaBoost classifier\n",
    "- how to evaluate the performacen of a classifier using cross-validation. \n",
    "\n",
    "In this tutorial, we form an ensemble of classifers that use different ML algorithms on the same dataset in a voting classifier. \n",
    "\n",
    "**Note that there is no data randomisation in a voting classifier.** \n",
    "\n",
    "Voting can be either hard or soft voting.\n",
    "\n",
    "We will also use a performace metrics, accuracy_score, to evaluate the performace of each individual classifier and the voting ensemble classifier. \n",
    "\n",
    "Fpr further information about the voting classifier, refer to: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples = 100, noise = 0.25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1) # the default split is 75:25\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc',svm_clf)], voting = 'hard')\n",
    "voting_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2. AdaBoost \n",
    "\n",
    "In this tutorial, we use the iris dataset provided by scikit-learn. For more information, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html?highlight=iris%20dataset\n",
    "\n",
    "https://en.wikipedia.org/wiki/Iris_flower_data_set\n",
    "\n",
    "For cross validation in sklearn, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n",
    "We will focus on a boosting ensemble method called AdaBoost. For further information on the ensemble methods in sklearn used in this tutorial, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "By default, weak learners in an AdaBoost classifier are decision stumps. Different weak learners can be specified through the base_estimator parameter, though the base classifier needs one of those that are able to return the probabilities with classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import both the dataset and the classifiers\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target # use all features\n",
    "\n",
    "# use the decision tree classifier\n",
    "clf = DecisionTreeClassifier() # to full depth\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further information about the AdaBoostClassifier, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the AdaBoost classifier with the default base classifier - DecisionTreeClassifier(max_depth=1) \n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "# the train-test split is done in each iteration of cross validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the AdaBoost classifier with the logistic regression classifier\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "\n",
    "clf = AdaBoostClassifier(log_clf,\n",
    "    n_estimators=100)\n",
    "\n",
    "# the train-test split is done in each iteration of cross validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())\n",
    "\n",
    "# use the AdaBoost classifier with the DecisionTreeClassifier(max_depth=1) and a learning_rate\n",
    "\n",
    "clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=100, learning_rate=1.5)\n",
    "\n",
    "# the train-test split is done in each iteration of cross validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the AdaBoost classifier with the random forest classifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier()\n",
    "\n",
    "clf = AdaBoostClassifier(rnd_clf,\n",
    "    n_estimators=100)\n",
    "\n",
    "# the train-test split is done in each iteration of cross validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "print(scores.mean())\n",
    "\n",
    "# use the AdaBoost classifier with the random forest classifier and a learning_rate\n",
    "\n",
    "clf = AdaBoostClassifier(rnd_clf,\n",
    "    n_estimators=100, learning_rate=1.5)\n",
    "\n",
    "# the train-test split is done in each iteration of cross validation\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1. Parameter tuning in the AdaBoostClassifier function\n",
    "\n",
    "As we have seen in the above tutorials there are parameters in the AdaBoostClassifier function which need to be tuned. Study the user guide on the function to undertand what these paramters are for and how their values can affect the classification results.\n",
    "\n",
    "- What does learning_rate represents? Run AdaBoostClassifier with the DecisionTreeClassifer as the base classifier on the iris dataset with learning_rate = 0.75 and compare the evaluation results of the ensemble with learning_rate = 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the remainder of this practical to work on either the individual assignment or the group project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
