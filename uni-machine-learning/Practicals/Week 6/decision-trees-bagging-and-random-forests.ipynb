{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 6\n",
    "\n",
    "## Tutorials and Exercises on Ensembles \n",
    "\n",
    "The objectives of this practical are to recap/learn:\n",
    "- the core concepts of decision trees\n",
    "- the core concepts of bagging\n",
    "- the core ceoncepts of randm forests\n",
    "- how to apply ensembles to solve ML problems\n",
    "- how to tune the appropriate hyperparameters when using ensembles with Scikit-Learn functionss for ensembles\n",
    "\n",
    "Some of the code used is adapted with additions/revisions from In-Depth: Decision Trees and Random Forests by Jake VanderPlas: https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n",
    "\n",
    "For further information on the ensemble methods in sklearn used in this tutorial, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: Decision Trees\n",
    "\n",
    "### Generate and plot a dataset\n",
    "\n",
    "We first generate and plot a two-dimensional dataset using make_blobs function in sklearn, with four classes of data points and plot it with different classes in different colours. \n",
    "\n",
    "Note that both make_blobs and make_classification create multiclass datasets by allocating each class one or more normally-distributed clusters of points. For more information on generating datasets using sklearn functions, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/sample_generators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=0, cluster_std=1.0) # n_features has a default value of 2\n",
    "\n",
    "# change the values of the parameters in make_blobs and see what happens\n",
    "\n",
    "# plot the dataset\n",
    "# marker has a default value of 'o', as many colours as class labels and a size of 50\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow'); \n",
    "\n",
    "# use marker='^' by uncommenting the following:\n",
    "#plt.scatter(X[:, 0], X[:, 1], c=y, s=50, marker='^', cmap='rainbow'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and visualise a decision tree classifier\n",
    "\n",
    "We can fit a simple decision tree classifier to this dataset, which will iteratively split the data along one or the other axis using a cut-off value as a quantitative criterion, and at each level assign the label of the new region according to a majority vote of data points within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function for visualising the output of a classifier\n",
    "\n",
    "This function takes as input a model, X, y, a colour map, fits the model to the dataset, and draw the decision boundary. \n",
    "\n",
    "**This is a very useful function that can be used for visualising the output of a classifier.**\n",
    "\n",
    "**Find out any of the functions or parameters that you don't know**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "    # Get the current Axes instance on the current figure matching the given keyword args, or create one.\n",
    "    ax = ax or plt.gca() \n",
    "    \n",
    "    # Plot the training points\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap,\n",
    "               clim=(y.min(), y.max()), zorder=3) \n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # fit the estimator\n",
    "    model.fit(X, y)\n",
    "    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n",
    "                         np.linspace(*ylim, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Create a color plot with the results\n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap=cmap, clim=(y.min(), y.max()),\n",
    "                           zorder=1)\n",
    "\n",
    "    ax.set(xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the output of the classifier to the maximum depth\n",
    "\n",
    "In this case, a decision tree is created as the classifier and visualised with no depth being specified explicitly. As we can see the decision tree is clearly over-fitting on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # import the decision tree classifier\n",
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the output of the classifier to different depths\n",
    "\n",
    "The figure below presents a visualisation of the output by a decision tree classifier fitted on this dataset to each of the first four depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(16, 3)) # create a figure with 4 axes\n",
    "fig.subplots_adjust(left=0.02, right=0.98, wspace=0.1)\n",
    "\n",
    "for axi, depth in zip(ax, range(1, 5)):\n",
    "    model = DecisionTreeClassifier(max_depth=depth) # the depth of a decision tree can be given\n",
    "    visualize_classifier(model, X, y, ax=axi)\n",
    "    axi.set_title('depth = {0}'.format(depth))\n",
    "\n",
    "fig.savefig('decision-tree-levels.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and draw a decision tree\n",
    "\n",
    "The decision tree can be fitted to a certain depth, e.g. 2, 3, or 4. \n",
    "\n",
    "Once trained, we can plot the tree with the plot_tree function. For further information on ftiing and plot decision trees, inclduing plotting the decision boundary and the tree itself,  refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "\n",
    "We can compare a tree with the corresponding visualisation of the classifier to better understand how the dataset is split at each split. For example, at depth = 2, the tree can split/classify the dataset into 3 classes. We can see the splitting values at the two levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=2).fit(X, y) # to a depth of 2\n",
    "\n",
    "# model = DecisionTreeClassifier(max_depth=3).fit(X, y) # to a depth of 3\n",
    "\n",
    "# model = DecisionTreeClassifier(max_depth=4).fit(X, y) # to a depth of 4\n",
    "\n",
    "tree.plot_tree(model, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decision Trees\n",
    "\n",
    "- What's the class label of each leaf node in the above decision tree? How is it decided?\n",
    "- How is the splitting feature selected at each splitting point?\n",
    "- What are the parameters in the DecisionTreeClassifier function? How do their values affect the output of the classifier? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Bagging\n",
    "\n",
    "### Ensembles of  Decison Tree classifiers: Bagging\n",
    "\n",
    "Bagging makes use of an ensemble of estimators, each of which overfits on a subset of the data, and aggregates the results to find a better classification.\n",
    "\n",
    "In this case, we have randomised the traning dataset by fitting each decision tree on a random subset of the training points at 80%. We have used the BaggingClassifier to create and ensemble of the decision trees.\n",
    "\n",
    "As we can see the classification results by the ensemble is slightly better than an individual decision tree because of the dataset randomisation, though not very much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "bag.fit(X, y)\n",
    "visualize_classifier(bag, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.  Bagging\n",
    "\n",
    "### Create a bagging classifier using the SVM as the base classifiers.\n",
    "\n",
    "Create a bagging classifier of SVM base classifiers that are fitted on different random subsets of the data, and compare the results with the bagging classifer of the decision trees above.\n",
    "\n",
    "### Answer the following questions:\n",
    "\n",
    "- What are the parameters in the BaggingClassifier function? How do their values affect the output of the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuorial 3. Random Forests\n",
    "\n",
    "In practice, in addition to data randomisation as when the bagging classifier is ued, another kind of randomisation can also be introduced by injecting some stochasticity in how the splitting features are chosen at the splits. For example, when determining which feature to split on, the randomised tree might select from among the top several features.\n",
    "\n",
    "In Scikit-Learn, such an optimised ensemble of randomized decision trees is implemented in the RandomForestClassifier estimator, which takes care of all the randomisation automatically. All you need to do is select a number of estimators, and it will very quickly fit the ensemble of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "visualize_classifier(model, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With aggregating over 100 randomly perturbed models, we end up with an overall model that is much closer to our intuition about how the space should be split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3. Parameter Tuning in bagging classifiers and random forest classifiers\n",
    "\n",
    "There are parameters in these classifiers that need to be tuned. When creating bagging classifers the main parameters are *n_estimators* and *max_samples*. When creating random forests the main parameters are *n_estimators*, *max_features* and *max_samples*. \n",
    "\n",
    "n_estimators is the number of base classifiers. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees in a random forest. \n",
    "\n",
    "max_samples is the size of the random subsets of data. The lower the size the greater reduction of variance, but also the greater increase in bias. \n",
    "\n",
    "max_features is the size of the random subsets of features to consider when splitting a node in a base decision tree classifier. The lower the greater the reduction of variance, but also the greater the increase in bias.\n",
    "\n",
    "In a random forest, the level of the decision trees can also be selected.\n",
    "\n",
    "Even in a voting classifier, the base classifiers can also be selected with different ensembles produce different levels of performance.\n",
    "\n",
    "There are also other paramters in these classifiers that can be tuned. For further information on the parameters in these classifiers, refer to information on each classifier in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/ensemble.html\n",
    "\n",
    "*In all the cases, the best parameter values should always be selected by cross validation*. For further information on performance evaluation, refer to model selection and evaluation in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/model_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# try different values of parameters\n",
    "# model = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", random_state=0)\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, max_features=\"log2\", random_state=0)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\", max_samples=0.6)\n",
    "\n",
    "visualize_classifier(model, X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: A Random Forest for Classifying Digits\n",
    "\n",
    "### Load a toy dataset\n",
    "\n",
    "We first load a toy dataset from sklearn. scikit-learn comes with a few small standard datasets that do not require to download any file from some external website.\n",
    "\n",
    "These datasets are useful to quickly illustrate the behaviour of the various algorithms implemented in scikit-learn. They are however often too small to be representative of real world machine learning tasks.\n",
    "\n",
    "For more information on the toy datasets in sklearn, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first few data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the digits using a random forest\n",
    "\n",
    "You can use cross_val_score to show a performance metrics. You can slect a different number of estimators and a different number of iterations in cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(model, digits.data, digits.target, cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to get a different score instead of the default one. For example, we can get the accuracy at each iteration of cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, digits.data, digits.target, scoring='accuracy', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to get a different score.\n",
    "\n",
    "For more information, refer to:\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, digits.data, digits.target, scoring='recall_micro', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(model, digits.data, digits.target, scoring='precision_micro', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4. Random Forests\n",
    "\n",
    "### Fit, use and evaluate a random forest classifier on the breast cancer dataset and compare with the decision tree model.\n",
    "\n",
    "This is another toy dataset in scikit-learn. For more information on the dataset, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/toy_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
