{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 5: SVM (Kernels)\n",
    "\n",
    "The objectives of this practical are to learn:\n",
    "- the core concepts of kernels\n",
    "- how to choose and use kernels\n",
    "- how to build a pipeline of an ML task\n",
    "- how to apply SVMs to solve ML problems\n",
    "- how to tune the appropriate hyperparameters when using SVMs with Scikit-Learn methods for SVMs\n",
    "\n",
    "### What are kernels?\n",
    "\n",
    "When the data points of different classes are not linearly separable in the original low dimensional space, a kernel function/trick can be used to calculate the distance between a pair of data points in a higher dimensional space so that a hyperplane (and decision boundary) can be formed in the higher dimensional space to linealy separate the data.\n",
    "\n",
    "Note that we don't actually generate the higher dimensional space but we use the kernel function/trick to calculate the distance between a pair of data points in the higher dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: Polynomial kernels\n",
    "\n",
    "Note that the polynomial kernel is defined as follows: $k(x^{(i)},x^{(j)})=((x^{(i)})^Tx^{(j)}+r)^d$. Refer to the lecture slides for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adopted from Hands-On Machine Learning with Scikit-Learn and TensorFlow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and display a dataset\n",
    "\n",
    "We first generate a dataset that is not linearly separatable. scikit-learn provides a number of random sample generators (e.g. make_moons() used in this tutorial) in the sklearn.datasets class that can be used to generate datasets of controlled size and complexity. These datasets are particularly useful for illustrations.\n",
    "\n",
    "For further information about the parameters used in the method, refer to:\n",
    "https://scikit-learn.org/stable/datasets/sample_generators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_moons(n_samples=100, noise=0.15, random_state=42)\n",
    "\n",
    "# This is a useful method for plotting a dataset for binary classification.\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:,0][y==0], X[:, 1][y==0], \"bs\")\n",
    "    plt.plot(X[:,0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True, which='both')\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting an SVM decision bounday\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                    model.support_vectors_[:, 1],\n",
    "                    s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and display the decision boundary fitted by a linear kernel\n",
    "\n",
    "For further information on the SVC method, refer to:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the linear decision boundary is clearly not able to separate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a pipeline\n",
    "\n",
    "SVMs are sensitive to the feature scales. We now build a pipeline to include the the StandardScaler() imported from sklearn.preprocessing class to scale the features before fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel='linear'))\n",
    "])\n",
    "\n",
    "linear_kernel_svm_clf.fit(X, y)\n",
    "\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plot_svc_decision_function(linear_kernel_svm_clf.fit(X, y), plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as we can see the linear decision boundary is clearly not able to separate the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a linear SVM classifier using additional polynomial features\n",
    "\n",
    "As we can see above, the data points are clearly not separatable in the original feature space using a linear SVM.\n",
    "\n",
    "We can form an additional polynomial feature so that the data points can be separated in the new feature space using a linear SVM. In this class the SVM classifier is actually a 2d plane which when projected back to the original feature space shows as a curved road.\n",
    "\n",
    "For further information on the SVC method, refer to:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=4, loss='hinge', random_state=42)) # C changed from 10 to 4 in order to converge\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary from the fitted model in the polynomial feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plot_svc_decision_function(polynomial_svm_clf.fit(X, y), plot_support=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see a soft margin classifier can be generated in the polynomical feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task. \n",
    "\n",
    "- Change the value of C in the LinearSVC model to see what happens. Can you compare the plot with the one generated when the value of C is set to 4 to see what the differences between them are? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a linear SVM classifier using a polynomial kernel\n",
    "Instead of creating an additional polynomial feature to form a higher dimensional feature space so that a linear SVM can be fitted, in this class we use a polynomial kernel which uses the polynomial kernel trick and does not need to actually creating the additional polynomial feature.\n",
    "\n",
    "In this case the SVM classifier is a 2d smooth plane which when projected back to the original feature space shows as a curved road, which is similar to the linear classifier we previously had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=4))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note the values of the hyper-parameters, degree $d$, coef0 for $r$ and $C$ for the softness of the classifier.**\n",
    "\n",
    "Refer to the lecture slides for the definitions of these hyper-parameters.\n",
    "\n",
    "For further information on the SVC method, refer to:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plot_svc_decision_function(poly_kernel_svm_clf.fit(X, y), plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the plot, the decision boundary generated by the polynal kernal is a good approximation of the decision boundary generated in the polynomial feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Radial Basis Function (RBF) Kernels\n",
    "\n",
    "We first generate another dataset that is not linearly separatable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats # has this been used?\n",
    "\n",
    "# use seaborn plotting methods\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decision bounday of the model\n",
    "\n",
    "We first define a function for plotting an SVM decision boundary.\n",
    "\n",
    "*Note that this is a useful function for plotting an SVM decision boundary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting an SVM decision bounday\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "\n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                    model.support_vectors_[:, 1],\n",
    "                    s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "\n",
    "X, y = datasets.make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have tried to find a linear classifier to separate the data points which the classifier clearly is not be able to.\n",
    "\n",
    "We now project the data to a higher dimensional space using the radial basis function (RBF) which basically lifting the data points in the middle to bent the 2-d data into 3-d data.\n",
    "\n",
    "Note that projecting data from a lower dimensional space to a higher dimensional space is to use a function of the variables in the lower dimension space to form an extra dimension in the higher dimensional space.\n",
    "\n",
    "The other way round, when we project data from a higher dimensional space into a lower dimensional space to assign a constant to a feature variable in one dimension so that the dimensionality can be reduced by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the data in a 3-d plot after it is projected to the third dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "def plot_3D(elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], z, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "\n",
    "interact(plot_3D, elev=[-90, 30, 90], azim=(-180, 180),\n",
    "         X=fixed(X), y=fixed(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with this extra dimension, $z$, the data becomes linearly separatable by a plane at, say, $z=0.7$. When we project the plane back to the original 2-d space, i.e., using $x$ and $y$ values of the plane only, we can draw a circle in the 2-d space to separate the data, i.e., yelow dots from red dots.\n",
    "\n",
    "This kind of functional transformation is called a kernel transformation. \n",
    "\n",
    "However, projecting $M$ data points into $N$ dimensions can become computationally expensive or even not feasible when $N$ grows large or even infinite.\n",
    "\n",
    "What we actually do in SVM is to use a kernel function called kernel trick to fit a SVM model (i.e. a hyperplane) in the transformed data **implicitly** (i.e. without explicitly transforming the data). Nevertheless, we can fit the SVM model in a high dimensional space as if we fit the model to the transformed data. Use of kernel tricks makes SVM very powerful yet computationally efficient. \n",
    "\n",
    "In Scikit-Learn, we can apply a kernelised SVM by choosing a non-linear kernel, e.g. RBF, using the kernel hyperparameter, $\\gamma$. Refer to: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', C=1E6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "# a very useful function for plotting the decision boundary projected back from the higher-dimensional space to\n",
    "# the lower-dimensional space\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the kernelised SVM, we can fit/learn a decision boundary in the high dimensinal space. When we project the decision boundary back to the original/low dimensional space, We can see the projected decision boundary in the 2-d space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise.\n",
    "Try different values of the appropriate hyper-parameters, $\\gamma$, $C$ and see what happens.\n",
    "\n",
    "Refer to the lecture slides for the defintions of these paramters.\n",
    "\n",
    "For further information on the SVC method, refer to:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
