{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical 4: SVM (Basics)\n",
    "\n",
    "The objectives of this practical are:\n",
    "- To learn intuitively with illustrations the basics of SVM with a focus on linear SVM\n",
    "- To better understand with illutations the concepts of maximal and soft margin classifiers\n",
    "- To get hands-on expereince in using the relevant libraries for SVM\n",
    "\n",
    "## Tutorial: SVM with Illustrations\n",
    "\n",
    "The code adapted with updates/revisions from In-Depth: Support Vector Machines by Jake VanderPlas:\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "\n",
    "# use seaborn plotting methods\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset\n",
    "\n",
    "scikit-learn includes various random sample generators (e.g. make_blobs() used in this tutorial) in the sklearn.datasets class that can be used to generate datasets of controlled size and complexity. These datasets are particularly useful for illustrations.\n",
    "\n",
    "For further information about the parameters used in the function, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/sample_generators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.6) # random_state=0 produces the same dataset in every run\n",
    "\n",
    "# marker has a default value of 'o', as many colours as class labels, a size of 50 and a colour map of autumn\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the data points are well separated. We can change the value of cluster_std to introduce some noise if needed.\n",
    "\n",
    "**Change the value of cluster_std to a higher or lower value, e.g. 1.2 or 0.3, which represents a higher or lower standard derivation of the clusters, and see what happens.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which line should be chosen as the decision boundary?\n",
    "\n",
    "Many lines can be drawn to devide the data into two groups but we need to choose the best as the decision boundary.\n",
    "\n",
    "Given a new data point (e.g., the one marked by the \"X\" in this plot), depending on which line is chosen as the decision boundary, it can be assigned a different class! So which line should be chosen as the decision boundary that is the best to class a new data point to the class it's likely to belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5) # generate the x axis\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for w1, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
    "    plt.plot(xfit, w1 * xfit + b, '-k')\n",
    "    \n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of numpy and matplotlib.pyplot methods are used in the above cell. It would be useful to explore how to use them. For further information, refer to:\n",
    "\n",
    "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The line with the widest margin is the best!\n",
    "\n",
    "The line in the middle has the widest margin and it's least likely to make a wrong classification (i.e. has the largest margin of error). This line is the marximal margin classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1, 3.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
    "\n",
    "for w1, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = w1 * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "\n",
    "- What are the margin, the maximal margin and the maximal margin classifier in an SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit an SVM model with the dataset\n",
    "\n",
    "We can fit an SVM model, i.e. a kernel, with the dataset to find the maximal margin classifier. Let's use a linear kernel (refer to the lecture notes) and set the C parameter (refer to the lecture notes) to a very large number.\n",
    "\n",
    "For further information on SVMs supported by sklearn, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "For further information on the SVC function, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.\n",
    "\n",
    "- What's the linear kernel?\n",
    "\n",
    "- What is C=1E10 meant to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the decision bounday of the model\n",
    "\n",
    "We first define a function for plotting an SVM decision boundary.\n",
    "\n",
    "*Note that this is a useful function for plotting an SVM decision boundary.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting an SVM decision bounday\n",
    "\n",
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0], \n",
    "                    model.support_vectors_[:, 1], \n",
    "                    s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the function to draw the boundary\n",
    "\n",
    "This is the maximal margin classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "plot_svc_decision_function(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that those data points at the edges of the margin are the support vectors of the model. We can get those support vectors which are in a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to know the attributes of a fitted SVM model\n",
    "\n",
    "We can get access to various attributes of a fitted model using the appropriate methods. For further information, refer to:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the model\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the values of weights $w_1$ and $w_2$ in the three equations of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the constant of the decision boundary\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the value of bias $b$ in the three equations of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the support vectors\n",
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the three support vectors that decide on the decison boundary. Can you find them in the illustration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot two models learned from the first 60 and 120 data points of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = datasets.make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use IPython's interactive widgets to do this interactively to see how the decision boundary changes with each chosen size for the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, fixed\n",
    "interact(plot_svm, N=[10, 50, 100, 200], ax=fixed(None));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.\n",
    "\n",
    "As shown above, we have now known the weights and bias in the decision boundary, based on which we can calculate and answer the following questions (refer back to the lecture notes if needed):\n",
    "\n",
    "- What's the decision boundary of the model? \n",
    "- How to claculate the intercepts of the decision boundary? \n",
    "- What are the intercepts on the vertical and horizontal axes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft margin classifiers for noise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can change the value of cluster_std to increase the standard derivation of the clusters hence introducing some noise.\n",
    "\n",
    "When we have a dataset as shown above, in which some data points of the two clusters are overlapping, if we fit the maximal margin classifier on the dataset, we may overfit the model and the decison boundary will be very tweaked with a very narrow margin. When the classifier is used to predict a new data point, it's possible for it to be on the wrong side of the margin hence misclassified.\n",
    "\n",
    "In this case, to fit a soft margin classifier on the dataset would be better in terms of balancing the classification error and the margin error.\n",
    "\n",
    "When fitting the model, we can change the $C$ parameter to affect the softness of the margin, the larger $C$ is, the harder the margin is; or the smaller $C$ is the softer the margin is.\n",
    "\n",
    "The plot below shows how a changing $C$ parameter affects the softness of the margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of the $C$ parameter will depend on your dataset and application requirements, which can be tuned using cross-validation. In order to tune $C$, a validation dataset is needed, which needs to be different from the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.\n",
    "\n",
    "- What are the margin error, the classification error and the soft margin classifier in an SVM?\n",
    "- How to balance the trade-off between the margin error and the classification error?\n",
    "- How is the SVM error defined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (optional). \n",
    "\n",
    "### Apply the linear kernel to the datasets we have used, e.g. the titanic, SUV purchase, the iris datasets.\n",
    "\n",
    "You can compare its performace of the linear SVM with the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine Learning",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
