{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject: Machine Learning, 1st March 2024 to 3rd May 2024.\n",
    "## Topic: Group Project (Task B)\n",
    "## Learning Outcomes:\n",
    "- MO2: Select and apply machine learning algorithms to formulate \n",
    "solutions to different types of machine learning problems, taking \n",
    "into account criteria such data availability and characteristics, and \n",
    "problem-specific requirements for balancing speed, accuracy, and \n",
    "explainability.\n",
    "\n",
    "- MO3: Implement and evaluate contemporary machine learning \n",
    "solutions to application problems using a range of contemporary \n",
    "frameworks.\n",
    "\n",
    "- MO4: Demonstrate an awareness of the ethical and societal \n",
    "implications of machine learning solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "current_directory = os.path.dirname(os.path.abspath(\"main.ipynb\"))\n",
    "current_directory += \"/\"\n",
    "\n",
    "def install(requirements):\n",
    "    \"\"\"\n",
    "    Install all the relevent project dependencies.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        if os.path.isdir('.venv'):\n",
    "            activate_script = os.path.join('.venv', 'bin', 'activate')\n",
    "            subprocess.check_call(['source', activate_script], shell=True)\n",
    "\n",
    "        with open(requirements, 'r') as f:\n",
    "            requirements = f.read().splitlines()\n",
    "            subprocess.check_call(['pip', 'install'] + requirements)\n",
    "        print(\"Installed dependencies.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{requirements}' not found.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    install(current_directory + \"project/requirements.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "raw_data = pd.read_csv(current_directory + \"project/raw_data/BTC-USD.csv\")\n",
    "df = raw_data[[\"Date\", \"Close\"]]\n",
    "\n",
    "def to_datetime(s: str):\n",
    "    year, month, day = s.split(\"-\")\n",
    "    return dt.datetime(\n",
    "        int(year),\n",
    "        int(month),\n",
    "        int(day)\n",
    "    )\n",
    "\n",
    "close = df[\"Close\"]\n",
    "dates = df[\"Date\"].apply(to_datetime)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning and Normalisation\n",
    "Get the training set, validation set and test set for the model to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Prepare X and y\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_close_price = 'ScaledClosePrice'\n",
    "df[scaled_close_price] = scaler.fit_transform(df[[\"Close\"]])\n",
    "\n",
    "lookback = 10  # The number of previous steps to include in our next prediction.\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(df) - lookback):\n",
    "    # Historical close prices\n",
    "    X.append(df[scaled_close_price].values[i:i+lookback])\n",
    "\n",
    "    # Future close price from dataset\n",
    "    y.append(df[scaled_close_price].values[i+lookback])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 0.6951469583048531 %\n",
      "Validation set: 0.14900888585099112 %\n",
      "Test set: 0.14900888585099112 %\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Split the data into Samples, steps, then features\n",
    "X = np.reshape(X, (X.shape[0], lookback, 1))\n",
    "\n",
    "train_size = 0.7\n",
    "test_size = 0.15\n",
    "\n",
    "# Split the data into train, test and validation\n",
    "# First, split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Calculate remaining size for validation after allocating test set\n",
    "validation_size = (1.0 - train_size - test_size) / (1.0 - test_size)\n",
    "\n",
    "# Split the remaining data into validation and new train sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(\n",
    "    X_train, y_train, test_size=validation_size, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Training set:\", len(X_train)/len(close), \"%\")\n",
    "print(\"Validation set:\", len(X_validation)/len(close), \"%\")\n",
    "print(\"Test set:\", len(X_test)/len(close), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # # Plot the training, validation, and test sets\n",
    "# plt.plot(dates[:len(X_train)], X_train, label='Training Set', color='blue')\n",
    "# plt.plot(dates[len(X_train):len(X_train) + len(X_validation)], X_validation, label='Validation Set', color='orange')\n",
    "# plt.plot(dates[len(X_train) + len(X_validation):], X_test, label='Test Set', color='green')\n",
    "\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Close Price')\n",
    "# plt.title(\"Cryptocurrency Market Prices\")\n",
    "# plt.legend()\n",
    "\n",
    "\n",
    "train_indices = range(len(X_train))\n",
    "test_indices = range(len(X_train), len(X_train) + len(X_test))\n",
    "validation_indices = range(len(X_train) + len(X_test), len(X_train) + len(X_test) + len(X_validation))\n",
    "\n",
    "# Plot train set\n",
    "plt.plot(train_indices, y_train, label='Train', color='blue')\n",
    "\n",
    "# Plot validation set\n",
    "plt.plot(validation_indices, y_validation, label='Validation', color='orange')\n",
    "\n",
    "\n",
    "# Plot test set\n",
    "plt.plot(test_indices, y_test, label='Test', color='green')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Train, Test, and Validation Sets')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualise the data:\n",
    "# target_variable = 'Next Close'\n",
    "# X_reshaped = np.reshape(X, (X.shape[0], -1))\n",
    "# X_df = pd.DataFrame(X_reshaped, columns=[f\"Close_{i}\" for i in range(X.shape[1])])\n",
    "# y_df = pd.DataFrame(y, columns=[target_variable])\n",
    "# data_df = pd.concat([X_df, y_df], axis=1)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from keras.api.models import Sequential\n",
    "    from keras.api.layers import LSTM\n",
    "except ImportError:\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM\n",
    "\n",
    "# Our LSTM units (neurons)\n",
    "n_samples = len(dates)-1\n",
    "timestep_delta = abs(dates.iloc[0].year - dates.iloc[-1].year)\n",
    "\n",
    "# Sequence length\n",
    "n_timesteps = int(n_samples / timestep_delta)\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    units: int,\n",
    "    activation: str = \"relu\",\n",
    "    recurrent_activation: str = \"sigmoid\",\n",
    "    optimizer: str = \"adam\"\n",
    ") -> Sequential:\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        units,\n",
    "        activation=activation,\n",
    "        recurrent_activation=recurrent_activation,\n",
    "        return_sequences=True,\n",
    "        input_shape=(n_timesteps, 1)\n",
    "    ))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm_model = create_model(\n",
    "    units=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# We use this object to monitor the loss of the model.\n",
    "# After 'patient' consecutive attempts if no improvement we stop running the model.\n",
    "# We use val_loss since we are fine-tuning with validation data.\n",
    "loss_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fit the model.\n",
    "# Since we dont have explicit labels for validation monitor performance\n",
    "# during training instead.\n",
    "model_data = lstm_model.fit(\n",
    "    X_train, ,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_validation, None),\n",
    "    callbacks=[loss_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Create a keras wrapper for sklearn grid search\n",
    "class KerasRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, func, **kwargs):\n",
    "        self.func = func\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        self.model = self.func(**kwargs)\n",
    "        self.model.fit(X, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "regressor_model = KerasRegressor(lstm_model, verbose=0)\n",
    "\n",
    "grid_search_params = {\n",
    "    \"units\": [32, 64, 128]\n",
    "}\n",
    "\n",
    "def create_grid_search():\n",
    "    model = GridSearchCV()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
